{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cbe432",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **22AIE401 - Reinforcement Learning**  \n",
    "# **Lab 3**  \n",
    "\n",
    "</center>\n",
    "\n",
    "### Team Members:\n",
    "- Guruprasath M R - AIE22015  \n",
    "- Rudraksh Mohanty - AIE22046  \n",
    "- Shree Prasad M - AIE22050  \n",
    "- Tharun Kaarthick G K - AIE22062  \n",
    "\n",
    "---\n",
    "\n",
    "### Objective:\n",
    "To model a real-world warehouse navigation problem as a Markov Decision Process (MDP) and solve it using Value Iteration to find the optimal path for a robot, minimizing delivery time and avoiding obstacles. \n",
    "\n",
    "---\n",
    "\n",
    "### Problem Statement:\n",
    "Warehouse Robot Path Optimization using Value Iteration \n",
    "In modern warehouses (like Amazon), robots move around grid-based layouts to pick and deliver \n",
    "packages. They must: \n",
    " - Avoid shelves (obstacles) \n",
    " - Take the shortest and safest route\n",
    " - Deliver the package to the goal location\n",
    " - Minimize collisions and redundant moves. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Common Interpretation after completing tasks:\n",
    "To be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9217f",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c994ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import mdptoolbox \n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "# Grid Parameters\n",
    "rows, cols = 5, 5\n",
    "num_states = rows * cols \n",
    "shelves = [(1, 1), (2, 2), (3, 3)] # Obstacle positions\n",
    "actions = ['up', 'down', 'left', 'right'] \n",
    "num_actions = len(actions) \n",
    "movement = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)} \n",
    "# Function to map (x, y) to index\n",
    "def to_index(x, y): \n",
    " return x * cols + y \n",
    "# Function to randomly set a goal not on a shelf\n",
    "def set_dynamic_goal(): \n",
    " possible = [(i, j) for i in range(rows) for j in range(cols) if (i, j) not\n",
    "in shelves] \n",
    " return random.choice(possible) \n",
    "# Randomly choose a goal position\n",
    "goal_state = set_dynamic_goal() \n",
    "print(\"  Current Goal Position:\", goal_state) \n",
    "# Transition and Reward Matrices\n",
    "P = [np.zeros((num_states, num_states)) for _ in range(num_actions)] \n",
    "R = np.zeros((num_states, num_actions)) \n",
    "# Build MDP with stochastic transitions (intended: 0.9, unintended: 0.1 split \n",
    "across others)\n",
    "for action_idx, action in enumerate(actions): \n",
    " dx, dy = movement[action] \n",
    " for x in range(rows): \n",
    " for y in range(cols): \n",
    " current_state = to_index(x, y) \n",
    " if (x, y) == goal_state: \n",
    " P[action_idx][current_state, current_state] = 1\n",
    " R[current_state, action_idx] = 10\n",
    " continue\n",
    " outcomes = [] \n",
    " # Intended move (90%)\n",
    " new_x, new_y = x + dx, y + dy \n",
    " if (new_x, new_y) in shelves or not (0 <= new_x < rows and 0 <= new_y \n",
    "< cols): \n",
    " new_state = current_state \n",
    " reward = -5 if (new_x, new_y) in shelves else -1\n",
    " else: \n",
    " new_state = to_index(new_x, new_y) \n",
    " reward = -1\n",
    " outcomes.append((new_state, 0.9, reward)) \n",
    " # 10% misstep (wrong move in any of the other 3 directions)\n",
    " other_actions = [a for i, a in enumerate(actions) if i != action_idx] \n",
    " for mis_action in other_actions: \n",
    " mx, my = movement[mis_action] \n",
    " new_x, new_y = x + mx, y + my \n",
    " if (new_x, new_y) in shelves or not (0 <= new_x < rows and 0 <= \n",
    "new_y < cols): \n",
    " mis_state = current_state \n",
    " mis_reward = -5 if (new_x, new_y) in shelves else -1\n",
    " else: \n",
    " mis_state = to_index(new_x, new_y) \n",
    " mis_reward = -1\n",
    " outcomes.append((mis_state, 0.1 / 3, mis_reward)) \n",
    " for s_next, prob, rew in outcomes: \n",
    " P[action_idx][current_state, s_next] += prob \n",
    " R[current_state, action_idx] += prob * rew # Expected reward\n",
    "# Run Value Iteration\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9) \n",
    "vi.run() \n",
    "# Reshape policy to grid\n",
    "policy_grid = np.array(vi.policy).reshape((rows, cols)) \n",
    "action_symbols = ['â†‘', 'â†“', 'â†', 'â†’'] \n",
    "policy_symbols = np.array([[action_symbols[a] for a in row] for row in\n",
    "policy_grid]) \n",
    "print(\"\\nðŸ“ Optimal Policy Grid:\") \n",
    "print(policy_symbols) \n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 6)) \n",
    "for x in range(rows): \n",
    " for y in range(cols): \n",
    " idx = to_index(x, y) \n",
    " if (x, y) == goal_state: \n",
    " plt.text(y, rows - x - 1, 'G', ha='center', va='center', fontsize=14, \n",
    "color='green') \n",
    " elif (x, y) in shelves: \n",
    " plt.text(y, rows - x - 1, 'S', ha='center', va='center', fontsize=14, \n",
    "color='red') \n",
    " else: \n",
    " plt.text(y, rows - x - 1, action_symbols[vi.policy[idx]], ha='center', \n",
    "va='center', fontsize=14) \n",
    "plt.xticks(range(cols)) \n",
    "plt.yticks(range(rows)) \n",
    "plt.grid(True) \n",
    "plt.title(\"Stochastic Optimal Policy with Dynamic Goal\") \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
