{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cbe432",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **22AIE401 - Reinforcement Learning**  \n",
    "# **Lab 4**  \n",
    "\n",
    "</center>\n",
    "\n",
    "### Team Members:\n",
    "- Guruprasath M R - AIE22015  \n",
    "- Rudraksh Mohanty - AIE22046  \n",
    "- Shree Prasad M - AIE22050  \n",
    "- Tharun Kaarthik G K - AIE22062  \n",
    "\n",
    "---\n",
    "\n",
    "### Objective:\n",
    "Design and implement a Monte Carlo-based learning agent that learns optimal policies for minimizing time to reach dynamic, weighted emergency locations under a probabilistic and time-varying urban environment. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Problem Statement:\n",
    "A taxi operates in a grid-based city (5x5). The driver needs to:\n",
    " - Pick up passengers from random locations.\n",
    " - Drop them at requested destinations.\n",
    " - Decide which direction to move in each state to maximize reward (successful trips).\n",
    " - Learn this policy without a known model (i.e., using Monte Carlo control) \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9217f",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e1037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymdptoolbox\n",
      "  Downloading pymdptoolbox-4.0-b3.zip (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\spras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pymdptoolbox) (1.25.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\spras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pymdptoolbox) (1.9.3)\n",
      "Building wheels for collected packages: pymdptoolbox\n",
      "  Building wheel for pymdptoolbox (setup.py): started\n",
      "  Building wheel for pymdptoolbox (setup.py): finished with status 'done'\n",
      "  Created wheel for pymdptoolbox: filename=pymdptoolbox-4.0b3-py3-none-any.whl size=25669 sha256=8f94a3f8450c5dede3d7a8ddf7e5aef2c501c8f61dace2185970995e5c681df7\n",
      "  Stored in directory: c:\\users\\spras\\appdata\\local\\pip\\cache\\wheels\\cc\\81\\b3\\db002373e7a93d9151e9dc9ea1084102b0028f2339724b32a3\n",
      "Successfully built pymdptoolbox\n",
      "Installing collected packages: pymdptoolbox\n",
      "Successfully installed pymdptoolbox-4.0b3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pymdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c994ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöë Training Complete: Smart Ambulance Dispatch Policy Learned.\n",
      "\n",
      "üìç Learned Ambulance Dispatch Policy Grid:\n",
      "‚Üí ‚Üí ‚Üê ‚Üê ‚Üê ‚Üë\n",
      "‚Üì ‚Üì ‚Üì S ‚Üì ‚Üí\n",
      "‚Üë ‚Üë ‚Üê ‚Üê ‚Üì ‚Üí\n",
      "‚Üí ‚Üê S ‚Üì ‚Üí ‚Üê\n",
      "‚Üí ‚Üë ‚Üí ‚Üë ‚Üê ‚Üê\n",
      "‚Üí ‚Üì ‚Üë ‚Üë ‚Üê ‚Üë\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# ---------- Environment Setup ----------\n",
    "GRID_SIZE = 6\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "ACTION_MAP = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "MAX_STEPS = 50\n",
    "DISCOUNT = 0.95\n",
    "EPSILON = 0.1\n",
    "EPISODES = 10000\n",
    "\n",
    "# Static obstacles (permanent roadblocks)\n",
    "static_obstacles = [(1, 3), (3, 2)]\n",
    "hospital = (0, 0)  # Ambulance dispatch center\n",
    "\n",
    "# Rush hour control\n",
    "def is_rush_hour(ep):\n",
    "    return ep % 1000 < 300 or ep % 1000 > 800  # Congested traffic windows\n",
    "\n",
    "# Emergency severity and urgency\n",
    "emergency_types = {\n",
    "    'minor': 20,\n",
    "    'moderate': 35,\n",
    "    'critical': 50\n",
    "}\n",
    "\n",
    "# Helper functions\n",
    "def is_valid(state):\n",
    "    x, y = state\n",
    "    return 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE\n",
    "\n",
    "def get_dynamic_obstacles():\n",
    "    return [(2, 4), (4, 1), (3, 3)] if random.random() < 0.3 else []\n",
    "\n",
    "def epsilon_greedy(state, Q):\n",
    "    if np.random.rand() < EPSILON or state not in Q:\n",
    "        return random.randint(0, len(ACTIONS) - 1)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def generate_emergency():\n",
    "    location = random.choice([\n",
    "        (i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)\n",
    "        if (i, j) != hospital and (i, j) not in static_obstacles\n",
    "    ])\n",
    "    severity = random.choice(list(emergency_types.keys()))\n",
    "    reward = emergency_types[severity]\n",
    "    return location, reward\n",
    "\n",
    "# ---------- Monte Carlo Training ----------\n",
    "Q = defaultdict(lambda: np.zeros(len(ACTIONS)))\n",
    "Returns = defaultdict(list)\n",
    "\n",
    "def run_episode(episode_num):\n",
    "    rush = is_rush_hour(episode_num)\n",
    "    prob_blocks = get_dynamic_obstacles()\n",
    "    all_obstacles = static_obstacles + prob_blocks\n",
    "    goal, goal_reward = generate_emergency()\n",
    "    state = hospital\n",
    "    episode = []\n",
    "    steps = 0\n",
    "\n",
    "    while steps < MAX_STEPS:\n",
    "        action_idx = epsilon_greedy(state, Q)\n",
    "        dx, dy = ACTION_MAP[ACTIONS[action_idx]]\n",
    "        next_state = (state[0] + dx, state[1] + dy)\n",
    "\n",
    "        if not is_valid(next_state) or next_state in all_obstacles:\n",
    "            reward = -10 if rush else -5\n",
    "            next_state = state\n",
    "        elif next_state == goal:\n",
    "            reward = goal_reward - steps\n",
    "        else:\n",
    "            reward = -2 if rush else -1\n",
    "\n",
    "        episode.append((state, action_idx, reward))\n",
    "\n",
    "        if next_state == goal:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    return episode\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "    episode = run_episode(ep)\n",
    "    G = 0\n",
    "    visited = set()\n",
    "    for t in reversed(range(len(episode))):\n",
    "        s, a, r = episode[t]\n",
    "        G = DISCOUNT * G + r\n",
    "        if (s, a) not in visited:\n",
    "            Returns[(s, a)].append(G)\n",
    "            Q[s][a] = np.mean(Returns[(s, a)])\n",
    "            visited.add((s, a))\n",
    "\n",
    "print(\"üöë Training Complete: Smart Ambulance Dispatch Policy Learned.\")\n",
    "\n",
    "# ---------- Policy Visualization ----------\n",
    "policy = np.full((GRID_SIZE, GRID_SIZE), '.', dtype=str)\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        state = (i, j)\n",
    "        if state in static_obstacles:\n",
    "            policy[i][j] = 'S'\n",
    "        elif state in Q:\n",
    "            best_action = np.argmax(Q[state])\n",
    "            policy[i][j] = ['‚Üë', '‚Üì', '‚Üê', '‚Üí'][best_action]\n",
    "        else:\n",
    "            policy[i][j] = ' '\n",
    "\n",
    "print(\"\\nüìç Learned Ambulance Dispatch Policy Grid:\")\n",
    "for row in policy:\n",
    "    print(' '.join(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ceb0d7",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Each hospital has a dynamic load (e.g., occupied beds). Ambulances should choose hospitals not just based on proximity but expected availability. Model hospital queues and incorporate delayed rewards based on treatment delay penalties. Train agents to learn which hospital is\n",
    "better not just closer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083e427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Training Complete.\n",
      "Learned hospital selection (sample): (0, 5)\n",
      "\n",
      "Task 1 Policy Grid:\n",
      "H ‚Üí ‚Üì ‚Üê ‚Üê H\n",
      "‚Üí ‚Üê ‚Üì ‚Üì ‚Üì ‚Üê\n",
      "‚Üí ‚Üì ‚Üí ‚Üê ‚Üê ‚Üì\n",
      "‚Üë ‚Üë ‚Üì ‚Üì ‚Üí ‚Üê\n",
      "‚Üê ‚Üí ‚Üë ‚Üë ‚Üê ‚Üë\n",
      "‚Üí ‚Üë ‚Üë ‚Üë ‚Üì H\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Multiple Hospitals with Dynamic Queues\n",
    "GRID_SIZE = 6\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "ACTION_MAP = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "MAX_STEPS = 50\n",
    "DISCOUNT = 0.95\n",
    "EPSILON = 0.1\n",
    "EPISODES = 5000\n",
    "\n",
    "# Multiple hospitals\n",
    "hospitals = [(0, 0), (5, 5), (0, 5)]\n",
    "hospital_queues = {h: 0 for h in hospitals}  # beds occupied\n",
    "max_beds = {h: 5 for h in hospitals}\n",
    "\n",
    "def update_hospital_queues():\n",
    "    for h in hospitals:\n",
    "        # Randomly discharge patients\n",
    "        if hospital_queues[h] > 0 and random.random() < 0.3:\n",
    "            hospital_queues[h] -= 1\n",
    "        # Randomly admit new patients\n",
    "        if hospital_queues[h] < max_beds[h] and random.random() < 0.2:\n",
    "            hospital_queues[h] += 1\n",
    "\n",
    "def choose_hospital():\n",
    "    # Prefer hospitals with available beds\n",
    "    available = [h for h in hospitals if hospital_queues[h] < max_beds[h]]\n",
    "    if available:\n",
    "        return random.choice(available)\n",
    "    return random.choice(hospitals)\n",
    "\n",
    "def run_episode_task1(ep):\n",
    "    update_hospital_queues()\n",
    "    goal, goal_reward = generate_emergency()\n",
    "    chosen_hospital = choose_hospital()\n",
    "    state = chosen_hospital\n",
    "    episode = []\n",
    "    steps = 0\n",
    "    while steps < MAX_STEPS:\n",
    "        action_idx = epsilon_greedy(state, Q)\n",
    "        dx, dy = ACTION_MAP[ACTIONS[action_idx]]\n",
    "        next_state = (state[0] + dx, state[1] + dy)\n",
    "        if not is_valid(next_state):\n",
    "            reward = -5\n",
    "            next_state = state\n",
    "        elif next_state == goal:\n",
    "            # Penalty if hospital queue is full\n",
    "            if hospital_queues[chosen_hospital] >= max_beds[chosen_hospital]:\n",
    "                reward = goal_reward - steps - 20  # delayed treatment penalty\n",
    "            else:\n",
    "                reward = goal_reward - steps\n",
    "        else:\n",
    "            reward = -1\n",
    "        episode.append((state, action_idx, reward))\n",
    "        if next_state == goal:\n",
    "            break\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    return episode\n",
    "\n",
    "# Train agent for Task 1\n",
    "Q = defaultdict(lambda: np.zeros(len(ACTIONS)))\n",
    "Returns = defaultdict(list)\n",
    "for ep in range(EPISODES):\n",
    "    episode = run_episode_task1(ep)\n",
    "    G = 0\n",
    "    visited = set()\n",
    "    for t in reversed(range(len(episode))):\n",
    "        s, a, r = episode[t]\n",
    "        G = DISCOUNT * G + r\n",
    "        if (s, a) not in visited:\n",
    "            Returns[(s, a)].append(G)\n",
    "            Q[s][a] = np.mean(Returns[(s, a)])\n",
    "            visited.add((s, a))\n",
    "print(\"Task 1 Training Complete.\")\n",
    "\n",
    "# Display learned hospital selection and policy grid\n",
    "chosen_hospital = choose_hospital()\n",
    "print(f\"Learned hospital selection (sample): {chosen_hospital}\")\n",
    "policy = np.full((GRID_SIZE, GRID_SIZE), '.', dtype=str)\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        state = (i, j)\n",
    "        if state in hospitals:\n",
    "            policy[i][j] = 'H'\n",
    "        elif state in Q:\n",
    "            best_action = np.argmax(Q[state])\n",
    "            policy[i][j] = ['‚Üë', '‚Üì', '‚Üê', '‚Üí'][best_action]\n",
    "        else:\n",
    "            policy[i][j] = ' '\n",
    "print(\"\\nTask 1 Policy Grid:\")\n",
    "for row in policy:\n",
    "    print(' '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801fac7",
   "metadata": {},
   "source": [
    "## Task 2: \n",
    "\n",
    "Update your environment so that traffic jams or roadblocks may appear after the episode has started. Modify your simulation to invalidate paths mid-way and require real-time policy adaptation using Monte Carlo rollouts. The agent should reroute to avoid costly delays. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a081de55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Training Complete.\n",
      "\n",
      "Task 2 Sample Episode (state, action, reward):\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 2, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 1, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 3, -1)\n",
      "((1, 1), 2, -1)\n",
      "((1, 0), 0, -1)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n",
      "((0, 0), 0, -10)\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Dynamic Obstacles Mid-Episode\n",
    "def get_dynamic_obstacles_midway(step):\n",
    "    # Obstacles appear after step 10\n",
    "    if step > 10 and random.random() < 0.2:\n",
    "        return [(random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1))]\n",
    "    return []\n",
    "\n",
    "def run_episode_task2(ep):\n",
    "    state = (0, 0)\n",
    "    goal, goal_reward = generate_emergency()\n",
    "    episode = []\n",
    "    steps = 0\n",
    "    obstacles = static_obstacles.copy()\n",
    "    while steps < MAX_STEPS:\n",
    "        obstacles += get_dynamic_obstacles_midway(steps)\n",
    "        action_idx = epsilon_greedy(state, Q)\n",
    "        dx, dy = ACTION_MAP[ACTIONS[action_idx]]\n",
    "        next_state = (state[0] + dx, state[1] + dy)\n",
    "        if not is_valid(next_state) or next_state in obstacles:\n",
    "            reward = -10\n",
    "            next_state = state\n",
    "        elif next_state == goal:\n",
    "            reward = goal_reward - steps\n",
    "        else:\n",
    "            reward = -1\n",
    "        episode.append((state, action_idx, reward))\n",
    "        if next_state == goal:\n",
    "            break\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    return episode\n",
    "\n",
    "# Train agent for Task 2\n",
    "Q = defaultdict(lambda: np.zeros(len(ACTIONS)))\n",
    "Returns = defaultdict(list)\n",
    "for ep in range(EPISODES):\n",
    "    episode = run_episode_task2(ep)\n",
    "    G = 0\n",
    "    visited = set()\n",
    "    for t in reversed(range(len(episode))):\n",
    "        s, a, r = episode[t]\n",
    "        G = DISCOUNT * G + r\n",
    "        if (s, a) not in visited:\n",
    "            Returns[(s, a)].append(G)\n",
    "            Q[s][a] = np.mean(Returns[(s, a)])\n",
    "            visited.add((s, a))\n",
    "print(\"Task 2 Training Complete.\")\n",
    "\n",
    "# Display sample episode with dynamic obstacles\n",
    "sample_ep = run_episode_task2(0)\n",
    "print(\"\\nTask 2 Sample Episode (state, action, reward):\")\n",
    "for step in sample_ep:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6630c6",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Model hospital occupancy as a time-varying parameter. An ambulance should decide not only the quickest path to an emergency but also the least crowded hospital for drop-off. Implement delayed penalties when the selected hospital has no immediate bed availability. Let the policy adapt over multiple episodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdc24fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 Training Complete.\n",
      "\n",
      "Task 3 Hospital Occupancy:\n",
      "Hospital (0, 0): 0 beds occupied\n",
      "Hospital (5, 5): 5 beds occupied\n",
      "Hospital (0, 5): 5 beds occupied\n",
      "\n",
      "Task 3 Policy Grid:\n",
      "H ‚Üí ‚Üí ‚Üê ‚Üí H\n",
      "‚Üí ‚Üì ‚Üì ‚Üì ‚Üí ‚Üê\n",
      "‚Üë ‚Üë ‚Üë ‚Üí ‚Üê ‚Üê\n",
      "‚Üí ‚Üí ‚Üê ‚Üí ‚Üê ‚Üì\n",
      "‚Üí ‚Üí ‚Üí ‚Üë ‚Üê ‚Üê\n",
      "‚Üê ‚Üì ‚Üë ‚Üë ‚Üë H\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Time-Varying Hospital Occupancy\n",
    "def update_hospital_occupancy():\n",
    "    for h in hospitals:\n",
    "        # Simulate time-varying occupancy\n",
    "        hospital_queues[h] = min(max_beds[h], max(0, hospital_queues[h] + random.choice([-1, 0, 1])))\n",
    "\n",
    "def run_episode_task3(ep):\n",
    "    update_hospital_occupancy()\n",
    "    goal, goal_reward = generate_emergency()\n",
    "    # Choose least crowded hospital\n",
    "    chosen_hospital = min(hospitals, key=lambda h: hospital_queues[h])\n",
    "    state = chosen_hospital\n",
    "    episode = []\n",
    "    steps = 0\n",
    "    while steps < MAX_STEPS:\n",
    "        action_idx = epsilon_greedy(state, Q)\n",
    "        dx, dy = ACTION_MAP[ACTIONS[action_idx]]\n",
    "        next_state = (state[0] + dx, state[1] + dy)\n",
    "        if not is_valid(next_state):\n",
    "            reward = -5\n",
    "            next_state = state\n",
    "        elif next_state == goal:\n",
    "            # Penalty if hospital is full\n",
    "            if hospital_queues[chosen_hospital] >= max_beds[chosen_hospital]:\n",
    "                reward = goal_reward - steps - 30\n",
    "            else:\n",
    "                reward = goal_reward - steps\n",
    "        else:\n",
    "            reward = -1\n",
    "        episode.append((state, action_idx, reward))\n",
    "        if next_state == goal:\n",
    "            break\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    return episode\n",
    "\n",
    "# Train agent for Task 3\n",
    "Q = defaultdict(lambda: np.zeros(len(ACTIONS)))\n",
    "Returns = defaultdict(list)\n",
    "for ep in range(EPISODES):\n",
    "    episode = run_episode_task3(ep)\n",
    "    G = 0\n",
    "    visited = set()\n",
    "    for t in reversed(range(len(episode))):\n",
    "        s, a, r = episode[t]\n",
    "        G = DISCOUNT * G + r\n",
    "        if (s, a) not in visited:\n",
    "            Returns[(s, a)].append(G)\n",
    "            Q[s][a] = np.mean(Returns[(s, a)])\n",
    "            visited.add((s, a))\n",
    "print(\"Task 3 Training Complete.\")\n",
    "\n",
    "# Display hospital occupancy and learned policy\n",
    "print(\"\\nTask 3 Hospital Occupancy:\")\n",
    "for h in hospitals:\n",
    "    print(f\"Hospital {h}: {hospital_queues[h]} beds occupied\")\n",
    "policy = np.full((GRID_SIZE, GRID_SIZE), '.', dtype=str)\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        state = (i, j)\n",
    "        if state in hospitals:\n",
    "            policy[i][j] = 'H'\n",
    "        elif state in Q:\n",
    "            best_action = np.argmax(Q[state])\n",
    "            policy[i][j] = ['‚Üë', '‚Üì', '‚Üê', '‚Üí'][best_action]\n",
    "        else:\n",
    "            policy[i][j] = ' '\n",
    "print(\"\\nTask 3 Policy Grid:\")\n",
    "for row in policy:\n",
    "    print(' '.join(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
