{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cbe432",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **22AIE401 - Reinforcement Learning**  \n",
    "# **Lab 4**  \n",
    "\n",
    "</center>\n",
    "\n",
    "### Team Members:\n",
    "- Guruprasath M R - AIE22015  \n",
    "- Rudraksh Mohanty - AIE22046  \n",
    "- Shree Prasad M - AIE22050  \n",
    "- Tharun Kaarthick G K - AIE22062  \n",
    "\n",
    "---\n",
    "\n",
    "### Objective:\n",
    "Design and implement a Monte Carlo-based learning agent that learns optimal policies for minimizing time to reach dynamic, weighted emergency locations under a probabilistic and time-varying urban environment. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Problem Statement:\n",
    "A taxi operates in a grid-based city (5x5). The driver needs to:\n",
    " - Pick up passengers from random locations.\n",
    " - Drop them at requested destinations.\n",
    " - Decide which direction to move in each state to maximize reward (successful trips).\n",
    " - Learn this policy without a known model (i.e., using Monte Carlo control) \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Common Interpretation after completing tasks:\n",
    "To be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9217f",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4e1037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymdptoolbox in /usr/local/lib/python3.11/dist-packages (4.0b3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pymdptoolbox) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pymdptoolbox) (1.15.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pymdptoolbox) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pymdptoolbox) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pymdptoolbox) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pymdptoolbox) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pymdptoolbox) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pymdptoolbox) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pymdptoolbox) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pymdptoolbox) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pymdptoolbox) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pymdptoolbox) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pymdptoolbox) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c994ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš‘ Training Complete: Smart Ambulance Dispatch Policy Learned.\n",
      "\n",
      "ğŸ“ Learned Ambulance Dispatch Policy Grid:\n",
      "â†’ â†“ â†“ â† â†“ â†‘\n",
      "â†’ â†’ â† S â†“ â†“\n",
      "â†’ â†‘ â†‘ â†’ â† â†’\n",
      "â† â†‘ S â†“ â†‘ â†‘\n",
      "â†’ â† â†‘ â†‘ â†‘ â†\n",
      "â† â†‘ â† â†“ â† â†“\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# ---------- Environment Setup ----------\n",
    "GRID_SIZE = 6\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "ACTION_MAP = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "MAX_STEPS = 50\n",
    "DISCOUNT = 0.95\n",
    "EPSILON = 0.1\n",
    "EPISODES = 10000\n",
    "\n",
    "# Static obstacles (permanent roadblocks)\n",
    "static_obstacles = [(1, 3), (3, 2)]\n",
    "hospital = (0, 0)  # Ambulance dispatch center\n",
    "\n",
    "# Rush hour control\n",
    "def is_rush_hour(ep):\n",
    "    return ep % 1000 < 300 or ep % 1000 > 800  # Congested traffic windows\n",
    "\n",
    "# Emergency severity and urgency\n",
    "emergency_types = {\n",
    "    'minor': 20,\n",
    "    'moderate': 35,\n",
    "    'critical': 50\n",
    "}\n",
    "\n",
    "# Helper functions\n",
    "def is_valid(state):\n",
    "    x, y = state\n",
    "    return 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE\n",
    "\n",
    "def get_dynamic_obstacles():\n",
    "    return [(2, 4), (4, 1), (3, 3)] if random.random() < 0.3 else []\n",
    "\n",
    "def epsilon_greedy(state, Q):\n",
    "    if np.random.rand() < EPSILON or state not in Q:\n",
    "        return random.randint(0, len(ACTIONS) - 1)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def generate_emergency():\n",
    "    location = random.choice([\n",
    "        (i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)\n",
    "        if (i, j) != hospital and (i, j) not in static_obstacles\n",
    "    ])\n",
    "    severity = random.choice(list(emergency_types.keys()))\n",
    "    reward = emergency_types[severity]\n",
    "    return location, reward\n",
    "\n",
    "# ---------- Monte Carlo Training ----------\n",
    "Q = defaultdict(lambda: np.zeros(len(ACTIONS)))\n",
    "Returns = defaultdict(list)\n",
    "\n",
    "def run_episode(episode_num):\n",
    "    rush = is_rush_hour(episode_num)\n",
    "    prob_blocks = get_dynamic_obstacles()\n",
    "    all_obstacles = static_obstacles + prob_blocks\n",
    "    goal, goal_reward = generate_emergency()\n",
    "    state = hospital\n",
    "    episode = []\n",
    "    steps = 0\n",
    "\n",
    "    while steps < MAX_STEPS:\n",
    "        action_idx = epsilon_greedy(state, Q)\n",
    "        dx, dy = ACTION_MAP[ACTIONS[action_idx]]\n",
    "        next_state = (state[0] + dx, state[1] + dy)\n",
    "\n",
    "        if not is_valid(next_state) or next_state in all_obstacles:\n",
    "            reward = -10 if rush else -5\n",
    "            next_state = state\n",
    "        elif next_state == goal:\n",
    "            reward = goal_reward - steps\n",
    "        else:\n",
    "            reward = -2 if rush else -1\n",
    "\n",
    "        episode.append((state, action_idx, reward))\n",
    "\n",
    "        if next_state == goal:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    return episode\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "    episode = run_episode(ep)\n",
    "    G = 0\n",
    "    visited = set()\n",
    "    for t in reversed(range(len(episode))):\n",
    "        s, a, r = episode[t]\n",
    "        G = DISCOUNT * G + r\n",
    "        if (s, a) not in visited:\n",
    "            Returns[(s, a)].append(G)\n",
    "            Q[s][a] = np.mean(Returns[(s, a)])\n",
    "            visited.add((s, a))\n",
    "\n",
    "print(\"ğŸš‘ Training Complete: Smart Ambulance Dispatch Policy Learned.\")\n",
    "\n",
    "# ---------- Policy Visualization ----------\n",
    "policy = np.full((GRID_SIZE, GRID_SIZE), '.', dtype=str)\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        state = (i, j)\n",
    "        if state in static_obstacles:\n",
    "            policy[i][j] = 'S'\n",
    "        elif state in Q:\n",
    "            best_action = np.argmax(Q[state])\n",
    "            policy[i][j] = ['â†‘', 'â†“', 'â†', 'â†’'][best_action]\n",
    "        else:\n",
    "            policy[i][j] = ' '\n",
    "\n",
    "print(\"\\nğŸ“ Learned Ambulance Dispatch Policy Grid:\")\n",
    "for row in policy:\n",
    "    print(' '.join(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ceb0d7",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Each hospital has a dynamic load (e.g., occupied beds). Ambulances should choose hospitals not just based on proximity but expected availability. Model hospital queues and incorporate delayed rewards based on treatment delay penalties. Train agents to learn which hospital is\n",
    "better not just closer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083e427d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5801fac7",
   "metadata": {},
   "source": [
    "## Task 2: \n",
    "\n",
    "Update your environment so that traffic jams or roadblocks may appear after the episode has started. Modify your simulation to invalidate paths mid-way and require real-time policy adaptation using Monte Carlo rollouts. The agent should reroute to avoid costly delays. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a081de55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca6630c6",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Model hospital occupancy as a time-varying parameter. An ambulance should decide not only the quickest path to an emergency but also the least crowded hospital for drop-off. Implement delayed penalties when the selected hospital has no immediate bed availability. Let the policy adapt over multiple episodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efdc24fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
