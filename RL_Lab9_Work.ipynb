{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58abac6",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **22AIE401 - Reinforcement Learning**  \n",
    "# **Lab 9**  \n",
    "\n",
    "</center>\n",
    "\n",
    "### Team Members:\n",
    "- Guruprasath M R - AIE22015  \n",
    "- Rudraksh Mohanty - AIE22046  \n",
    "- Shree Prasad M - AIE22050  \n",
    "- Tharun Kaarthik G K - AIE22062  \n",
    "\n",
    "---\n",
    "\n",
    "### Objective:\n",
    "Transfer Learning in Autonomous Farming To demonstrate the power of Transfer Learning in reinforcement learning by adapting a crop-monitoring robot’s navigation policy from one field (Field A) to a new field (Field B) with different crop layouts and hazards. The goal is to minimize time and damage while scanning all rows and reaching the base station.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Problem Statement:\n",
    "An autonomous robot has been trained to navigate Field A, which is organized with evenly spaced crop rows and water puddles (hazards). The robot must learn the most efficient route to: Visit all inspection checkpoints (marked on certain crops), Avoid water puddles, Reach the base station to upload data. Now, the robot is transferred to Field B, where: Crop rows are curved or uneven,  New puddles and rocks appear. The base station is in a different location.\n",
    " \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 8\n",
    "CHECKPOINTS_A = {(2, 2), (4, 4), (6, 1)}\n",
    "CHECKPOINTS_B = {(1, 3), (5, 5), (6, 2)}\n",
    "PUDDLES_A = {(3, 3), (5, 5), (2, 6)}\n",
    "PUDDLES_B = {(2, 2), (4, 5), (3, 6)}\n",
    "OBSTACLES = {(1, 1), (6, 6)}\n",
    "BASE_A = (7, 7)\n",
    "BASE_B = (0, 7)\n",
    "ACTIONS = ['U', 'D', 'L', 'R']\n",
    "ACTION_MAP = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}\n",
    "\n",
    "# Q-learning parameters\n",
    "EPISODES = 300\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.9\n",
    "EPSILON = 0.2\n",
    "MAX_STEPS = 200\n",
    "\n",
    "def is_valid(state):\n",
    "    x, y = state\n",
    "    return 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE and state not in OBSTACLES\n",
    "\n",
    "def step(state, action, config, visited_checkpoints):\n",
    "    dx, dy = ACTION_MAP[action]\n",
    "    next_state = (state[0] + dx, state[1] + dy)\n",
    "    if not is_valid(next_state):\n",
    "        next_state = state\n",
    "    reward = -1\n",
    "    if next_state in config[\"puddles\"]:\n",
    "        reward = -5\n",
    "    elif next_state in config[\"checkpoints\"] and next_state not in visited_checkpoints:\n",
    "        reward = 5\n",
    "        visited_checkpoints.add(next_state)\n",
    "    elif next_state == config[\"base\"] and visited_checkpoints == config[\"checkpoints\"]:\n",
    "        reward = 10\n",
    "    return next_state, reward, visited_checkpoints\n",
    "\n",
    "def select_action(Q, state):\n",
    "    if np.random.rand() < EPSILON or state not in Q:\n",
    "        return np.random.choice(ACTIONS)\n",
    "    return max(Q[state], key=Q[state].get)\n",
    "\n",
    "def train(config, Q=None):\n",
    "    if Q is None:\n",
    "        Q = {}\n",
    "    for ep in range(EPISODES):\n",
    "        state = (0, 0)\n",
    "        visited_checkpoints = set()\n",
    "        for _ in range(MAX_STEPS):\n",
    "            if state not in Q:\n",
    "                Q[state] = {a: 0 for a in ACTIONS}\n",
    "            action = select_action(Q, state)\n",
    "            next_state, reward, visited_checkpoints = step(state, action, config, visited_checkpoints)\n",
    "            if next_state not in Q:\n",
    "                Q[next_state] = {a: 0 for a in ACTIONS}\n",
    "            Q[state][action] += ALPHA * (\n",
    "                reward + GAMMA * max(Q[next_state].values()) - Q[state][action]\n",
    "            )\n",
    "            if next_state == config[\"base\"] and visited_checkpoints == config[\"checkpoints\"]:\n",
    "                break\n",
    "            state = next_state\n",
    "    return Q\n",
    "\n",
    "# Configurations\n",
    "config_A = {\"checkpoints\": CHECKPOINTS_A, \"puddles\": PUDDLES_A, \"base\": BASE_A}\n",
    "config_B = {\"checkpoints\": CHECKPOINTS_B, \"puddles\": PUDDLES_B, \"base\": BASE_B}\n",
    "\n",
    "# Train in Field A\n",
    "print(\"Training in Field A...\")\n",
    "Q_A = train(config_A)\n",
    "\n",
    "# Save and load Q-table\n",
    "with open(\"Q_fieldA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q_A, f)\n",
    "with open(\"Q_fieldA.pkl\", \"rb\") as f:\n",
    "    Q_loaded = pickle.load(f)\n",
    "\n",
    "# Transfer to Field B\n",
    "print(\"Transferring to Field B...\")\n",
    "Q_transfer = train(config_B, Q=Q_loaded)\n",
    "\n",
    "# Enhanced visualization function\n",
    "def plot_policy(Q, config, title):\n",
    "    grid = np.full((GRID_SIZE, GRID_SIZE), '.', dtype='<U1')\n",
    "    arrows = {'U': '↑', 'D': '↓', 'L': '←', 'R': '→'}\n",
    "    \n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            pos = (i, j)\n",
    "            if pos in OBSTACLES:\n",
    "                grid[i][j] = '■'\n",
    "            elif pos in config[\"puddles\"]:\n",
    "                grid[i][j] = '~'\n",
    "            elif pos in config[\"checkpoints\"]:\n",
    "                grid[i][j] = '✓'\n",
    "            elif pos == config[\"base\"]:\n",
    "                grid[i][j] = '⌂'\n",
    "            elif pos in Q:\n",
    "                best_a = max(Q[pos], key=Q[pos].get)\n",
    "                grid[i][j] = arrows[best_a]\n",
    "    \n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"Legend: ■=Obstacle, ~=Puddle, ✓=Checkpoint, ⌂=Base, Arrows=Policy\")\n",
    "    for row in grid:\n",
    "        print(' '.join(f'{cell:>2}' for cell in row))\n",
    "    \n",
    "    # Create beautiful matplotlib visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Create color-coded table\n",
    "    colors = []\n",
    "    for i in range(GRID_SIZE):\n",
    "        row_colors = []\n",
    "        for j in range(GRID_SIZE):\n",
    "            pos = (i, j)\n",
    "            if pos in OBSTACLES:\n",
    "                row_colors.append('#808080')  # Gray\n",
    "            elif pos in config[\"puddles\"]:\n",
    "                row_colors.append('#4FC3F7')  # Light Blue\n",
    "            elif pos in config[\"checkpoints\"]:\n",
    "                row_colors.append('#81C784')  # Light Green\n",
    "            elif pos == config[\"base\"]:\n",
    "                row_colors.append('#F48FB1')  # Light Pink\n",
    "            else:\n",
    "                row_colors.append('#FFFFFF')  # White\n",
    "        colors.append(row_colors)\n",
    "    \n",
    "    table = ax.table(cellText=grid, loc='center', cellLoc='center', \n",
    "                     cellColours=colors, bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    \n",
    "    # Style the table\n",
    "    for (i, j), cell in table.get_celld().items():\n",
    "        cell.set_linewidth(2)\n",
    "        cell.set_edgecolor('#333333')\n",
    "        cell.set_text_props(weight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_policy(Q, config, title=\"Policy Visualization\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.set_xlim(0, GRID_SIZE)\n",
    "    ax.set_ylim(0, GRID_SIZE)\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(np.arange(0, GRID_SIZE+1, 1))\n",
    "    ax.set_yticks(np.arange(0, GRID_SIZE+1, 1))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xlabel('X Coordinate', fontsize=12)\n",
    "    ax.set_ylabel('Y Coordinate', fontsize=12)\n",
    "    \n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            state = (i, j)\n",
    "            facecolor = 'white'\n",
    "            alpha = 0.8\n",
    "            \n",
    "            if state in OBSTACLES:\n",
    "                facecolor = 'gray'\n",
    "            elif state in config[\"puddles\"]:\n",
    "                facecolor = 'lightblue'\n",
    "            elif state in config[\"checkpoints\"]:\n",
    "                facecolor = 'lightgreen'\n",
    "            elif state == config[\"base\"]:\n",
    "                facecolor = 'lightcoral'\n",
    "            \n",
    "            rect = patches.Rectangle((j, GRID_SIZE-i-1), 1, 1, \n",
    "                                   linewidth=1.5, edgecolor='black', \n",
    "                                   facecolor=facecolor, alpha=alpha)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add text labels\n",
    "            if state in OBSTACLES:\n",
    "                ax.text(j+0.5, GRID_SIZE-i-0.5, 'OBS', ha='center', va='center', fontweight='bold')\n",
    "            elif state in config[\"puddles\"]:\n",
    "                ax.text(j+0.5, GRID_SIZE-i-0.5, 'PUD', ha='center', va='center', fontweight='bold')\n",
    "            elif state in config[\"checkpoints\"]:\n",
    "                ax.text(j+0.5, GRID_SIZE-i-0.5, 'CHK', ha='center', va='center', fontweight='bold')\n",
    "            elif state == config[\"base\"]:\n",
    "                ax.text(j+0.5, GRID_SIZE-i-0.5, 'BASE', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "            # Add policy arrows\n",
    "            if state in Q:\n",
    "                best_action = max(Q[state], key=Q[state].get)\n",
    "                dx, dy = {'U': (0, 0.3), 'D': (0, -0.3), 'L': (-0.3, 0), 'R': (0.3, 0)}[best_action]\n",
    "                ax.arrow(j + 0.5, GRID_SIZE - i - 0.5, dx, dy, \n",
    "                        head_width=0.15, head_length=0.1, fc='red', ec='red', \n",
    "                        linewidth=2, alpha=0.8)\n",
    "    \n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot policies with enhanced visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POLICY VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plot_policy(Q_A, config_A, \"Field A Policy (Original Training)\")\n",
    "plot_policy(Q_transfer, config_B, \"Policy after Transfer Learning in Field B\")\n",
    "\n",
    "# Display policy and table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Q-TABLE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = pd.DataFrame.from_dict({k: max(v, key=v.get) for k, v in Q_transfer.items()}, \n",
    "                           orient='index', columns=['Best Action'])\n",
    "print(\"Best Actions for each state in Field B (Transfer Learning):\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"\\nTotal states learned: {len(Q_transfer)}\")\n",
    "print(f\"Action distribution:\")\n",
    "action_counts = df['Best Action'].value_counts()\n",
    "for action, count in action_counts.items():\n",
    "    print(f\"  {action}: {count} states ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Enhanced grid visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED GRID VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "visualize_policy(Q_A, config_A, title=\"Autonomous Farming Policy in Field A\")\n",
    "visualize_policy(Q_transfer, config_B, title=\"Autonomous Farming Transfer Policy in Field B\")\n",
    "\n",
    "print(\"\\nVisualization Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb82237",
   "metadata": {},
   "source": [
    "### TASK-1\n",
    "Update the environment so that survivors randomly change locations every few episodes. The robot must relearn paths efficiently using prioritized sweeping instead of starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ed24e",
   "metadata": {},
   "source": [
    "### TASK-2\n",
    "Design certain cells to turn into traps mid-episode to simulate collapsing floors. Add a probabilistic element to cell stability, forcing the robot to quickly re-prioritize states with high error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbeeb0",
   "metadata": {},
   "source": [
    "### TASK-3\n",
    "Extend the setup to multiple rescue robots. Each robot learns independently but shares knowledge of dangerous zones. Coordinate their learning to minimize path overlap and maximize coverage. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
